{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9MqC-OwPKyQ"
   },
   "source": [
    "# Today you are a Machine Learning Engineer in the Department of Marketing and Inventory @ Walmart Labs.\n",
    "This work relies on processed data from Kaggle https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
    "\n",
    "This work is motivated by the publication https://arxiv.org/pdf/2010.02503.pdf\n",
    "\n",
    "*You have access to the Walmart server data, specifically the Electronics section, such that you have NO customer facing information, but you do have access to timestamped data regarding product viewing/carting/purchasing. We will use this data to build a model of whether a not some product will be purchased.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1wQKuWrOq9w"
   },
   "source": [
    "### If running this notebook in Google Colab, run the following cell first to mount your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NliK7yTn8LMn"
   },
   "source": [
    "^^ This mounts your Google Drive at the location */content/drive* on the virtual machine running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dR-vbsrRTv_"
   },
   "source": [
    "## Task 1: Read in the data\n",
    "\n",
    "You should have four files:\n",
    "\n",
    "* `some_column_descriptions.png`\n",
    "* `user_journey_descriptions.png`\n",
    "* `X_train.csv`\n",
    "* `X_test.csv`\n",
    "\n",
    "We'll start by looking at the descriptions before loading in the csv files.\n",
    "\n",
    "1. Use the `IPython.display` module to view the `some_column_descriptions.png` file.\n",
    "\n",
    "    Look through the column names and descriptions to get an idea of what the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "executionInfo": {
     "elapsed": 1253,
     "status": "ok",
     "timestamp": 1628175773190,
     "user": {
      "displayName": "Spencer Kent",
      "photoUrl": "",
      "userId": "07841346171340846448"
     },
     "user_tz": 360
    },
    "id": "h9EE7PccRRc9",
    "outputId": "d0c4ead6-d26a-465a-ea5f-140913936192"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Datasets/week_2/some_column_descriptions.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# change the filename to wherever you downloaded/uploaded the file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/My Drive/Datasets/week_2/some_column_descriptions.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m display(\u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/fourthbrain-ai/lib/python3.10/site-packages/IPython/core/display.py:952\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/fourthbrain-ai/lib/python3.10/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/fourthbrain-ai/lib/python3.10/site-packages/IPython/core/display.py:987\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m--> 987\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/fourthbrain-ai/lib/python3.10/site-packages/IPython/core/display.py:352\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_flags\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Datasets/week_2/some_column_descriptions.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "# change the filename to wherever you downloaded/uploaded the file\n",
    "filename = '/content/drive/My Drive/Datasets/week_2/some_column_descriptions.png'\n",
    "display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzZiHeyvPKyW"
   },
   "source": [
    "The dataset has User-journey data, i.e. it tracks information user/product pairs over time to see if the combination results in a purchase.\n",
    "\n",
    "2. Look at the `user_journey_descriptions.png` file.\n",
    "\n",
    "    Review the data sample to get a sense of what information we are tracking for each user/product pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1628175803603,
     "user": {
      "displayName": "Spencer Kent",
      "photoUrl": "",
      "userId": "07841346171340846448"
     },
     "user_tz": 360
    },
    "id": "MU87_Tt-TLff",
    "outputId": "629943bc-95f5-4d92-a88a-697dc7b75535"
   },
   "outputs": [],
   "source": [
    "# change path to wherever you uploaded/downloaded the file\n",
    "filename='/content/drive/My Drive/Datasets/week_2/user_journey_descriptions.png'\n",
    "display(Image(filename=filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSNM0etdU6HR"
   },
   "source": [
    "The dataset we are working with is essentially what we have screenshotted above, but has been anonomized by removing product IDs and user IDs.\n",
    "\n",
    "3. Use the pandas `read_csv()` and `head()` functions to read in the training data (`X_train.csv`) and look at the first few rows.\n",
    "\n",
    "    Note the Purchase column has either 0 (not purchased) or 1 (purchased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siqDCFPrQsG0"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "train_df = pd.read_csv(\"X_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Similarly, read in the test data (`X_test.csv`) and look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0kx76UCQzQn"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "test_df = pd.read_csv(\"X_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JunsLTrHPKyZ"
   },
   "source": [
    "## Task 2: Understand the Data\n",
    "\n",
    "*Our goal in this assignment is to predict whether a customer/product pair will result in a purchase. As part of this, we will want to identify what features are most important for making this classification.*\n",
    "\n",
    "We'll start with digging into our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xee_TNNyYHSy"
   },
   "source": [
    "1. For the training data, print out the datatype of each feature (column), and identify which ones are non-numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2QfKLnzYXqA"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "# Here are the numeric columns \n",
    "train_df.select_dtypes(include=[np.number]).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the non-numeric ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select_dtypes(include=['object']).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the unique values for the year, month and weekend features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "print(test_df[[\"Weekend\", \"year\", \"month\"]].nunique())\n",
    "print()\n",
    "print(train_df[[\"Weekend\", \"year\", \"month\"]].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Since these columns each only have one unique value, they will not be valuable features, so drop the three columns in both the training set and the test set.\n",
    "\n",
    "    Make sure to use `inplace=True` to modify the DataFrame.\n",
    "    \n",
    "    Print the shape of the DataFrames to verify the columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "train_df.drop([\"Weekend\", \"year\", \"month\"], inplace=True, axis=1)\n",
    "test_df.drop([\"Weekend\", \"year\", \"month\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhnGXhKlY00_"
   },
   "source": [
    "4. For each non-numeric feature, print the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIz5JdgYY6-x"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "def print_unique_values(df, columns):\n",
    "    print(df[columns].apply(lambda col: col.unique()))\n",
    "    \n",
    "columns =  [\"weekday\",\"timeOfDay\"]\n",
    "print(\"for train_df\\n\")\n",
    "print_unique_values(train_df, columns)\n",
    "\n",
    "print(\"\\n**** for test_df ******\")\n",
    "print_unique_values(test_df, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LzuyL8gZXNy"
   },
   "source": [
    "5. Convert the non-numeric features to numeric. These feature values are ordered temporally, so this makes some sense to do.\n",
    "\n",
    "    Follow the example given for the weekday column to update the dayOfWeek column.\n",
    "    \n",
    "    Use the `.head()` method to inspect the dataset after the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN503xCOPKyf"
   },
   "outputs": [],
   "source": [
    "def week_days_to_int(df): \n",
    "    weekday_strings = ['Mon', 'Tue', 'Wed', 'Thu', 'Fr', 'Sat', 'Sun']\n",
    "    weekday_ints = [1, 2, 3, 4, 5, 6, 7]\n",
    "    df['weekday'] = df['weekday'].replace(weekday_strings, weekday_ints)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = week_days_to_int(train_df) \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to encode timeofDya \n",
    "def encode_time_of_day(df):\n",
    "    time_of_day_strings = ['Dawn', 'EarlyMorning', 'Morning', 'Noon', 'Afternoon', 'Evening','Night', 'Dusk']\n",
    "    time_of_day_ints = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    df['timeOfDay'] = df['timeOfDay'].replace(time_of_day_strings, time_of_day_ints)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = encode_time_of_day(train_df)\n",
    "train_df['timeOfDay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = encode_time_of_day(test_df)\n",
    "test_df['timeOfDay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMMd7xjUZrqj"
   },
   "source": [
    "6. Do the same conversions for the test set.\n",
    "\n",
    "    Make sure to inspect the dataset to make sure it look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1eye5KePKym"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "test_df = week_days_to_int(test_df) \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5UZsme3bSpN"
   },
   "source": [
    "7. Use the `Purchase` columns of train and test datasets to determine the proportion of user journeys which result in purchases, in both the training and test datasets.\n",
    "\n",
    "    Are the datasets balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Purchase.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mab-Q3lKPKyo"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "sns.histplot(data=train_df, x=\"Purchase\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.Purchase.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHkECAjyPKyq"
   },
   "source": [
    "## Task 3: Reduce the number of features\n",
    "\n",
    "1. How many features does our dataset currently have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_4jlj1DPKyq"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "len(train_df.columns) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Draw a heatmap of the Pearson correlation.\n",
    "\n",
    "    The plotting code is given to you, but you need to fill in the calculation of the Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTbg-6-QPKyt"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cor = train_df.corr()\n",
    "sns.heatmap(cor, annot=True, fmt='.2g',vmin=-1, vmax=1, center= 0,  cmap= 'coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV2VP1qRcU1O"
   },
   "source": [
    "3. Drop the features with high correlation.\n",
    "\n",
    "    We'll do this by looking at each pair of features, and if they are highly correlated (at least 0.8), we won't include the second feature in the pair.\n",
    "\n",
    "    Store the remaining set of features (the ones you didn't drop) in dataframes `train_df_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map features to their absolute correlation values\n",
    "def get_correlated_columns(df, threshold=0.8):\n",
    "    corr = df.corr().abs()\n",
    "    # set equality (self correlation) as zero\n",
    "    corr[corr == 1] = 0\n",
    "    # of each feature, find the max correlation\n",
    "    # and sort the resulting array in ascending order\n",
    "    corr_cols = corr.max().sort_values(ascending=False)\n",
    "    \n",
    "    correlated_columns = list(corr_cols[corr_cols > threshold].index)\n",
    "    columns_to_drop = correlated_columns[::2]\n",
    "    print(columns_to_drop)\n",
    "    df_ = df.drop(columns_to_drop, axis = 1) \n",
    "    \n",
    "    return df_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "print(len(train_df.columns))\n",
    "train_df_reduced = get_correlated_columns(train_df)\n",
    "print(len(train_df_reduced.columns))\n",
    "train_df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many columns are left in the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "len(train_df_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove the same features from the test set and save in a new dataframe `test_df_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "print(len(test_df.columns))\n",
    "columns_to_drop = ['maxPrice', 'NumCart', 'NumOfEventsInJourney']\n",
    "test_df_reduced = test_df.drop(columns_to_drop, axis=1)\n",
    "print(len(test_df_reduced.columns))\n",
    "test_df_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytOeqOQOd8Fu"
   },
   "source": [
    "6. Visualize the selected features. (Ignore the warning messages about distplot.)\n",
    "    \n",
    "    If you have time, experiment with some other visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zr7RYNyPKy5"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 25))\n",
    "j = 0\n",
    "for i in train_df_reduced.columns:\n",
    "    plt.subplot(3, 4, j + 1)\n",
    "    j += 1\n",
    "    sns.histplot(train_df_reduced[i][train_df_reduced['Purchase'] == 0], color='g', label='no')\n",
    "    sns.histplot(train_df_reduced[i][train_df_reduced['Purchase'] == 1], color='r', label='yes')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Subscription Feature Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3wp3Fs1edry"
   },
   "source": [
    "7. Make the NumPy arrays `X_train_reduced`, `X_test_reduced`, `y_train` and `y_test` from `train_df_reduced` and `test_df_reduced`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY1KcmH_PKy7"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "X_train_reduced = train_df_reduced.values[:, 0:8]\n",
    "y_train = train_df_reduced.values[:, 8:]\n",
    "\n",
    "X_test_reduced = test_df_reduced.values[:, 0:8]\n",
    "y_test = test_df_reduced.values[:, 8:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw_mDww5htd6"
   },
   "source": [
    "8. Scale the features in `X_train_reduced` and `X_test_reduced` so that the max norm of each feature is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EDJeyj5hdgr"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize  # you can use this module, (but you don't have to)\n",
    "\n",
    "# [YOUR CODE HERE]\n",
    "X_train_reduced_normalized = normalize(X_train_reduced, norm='max', axis=0)\n",
    "X_test_reduced_normalized = normalize(X_test_reduced, norm='max', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9aXqA5MirW7"
   },
   "source": [
    "In the cell below we show you how to use a Random Forest to rank features based on a measure of importance called the Gini Importance. We'll provide you with all the code, but you may want to read up on sklearn's [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and its [feature_importances_](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier.feature_importances_) property.\n",
    "\n",
    "9. Look at the results below to note the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo5IeVYGPKy_"
   },
   "outputs": [],
   "source": [
    "# Use Random Forest to get feature ranks/importances for each feature\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Build a forest and compute the impurity-based feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=20,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X_train_reduced, y_train.ravel())\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train_reduced.shape[1]):\n",
    "    print(\"%d. %s (feature %d) (%f)\" %\n",
    "          (f + 1, train_df_reduced.columns[indices[f]], indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train_reduced.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train_reduced.shape[1]), indices)\n",
    "plt.xlim([-1, X_train_reduced.shape[1]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIJA9g5oPKzC"
   },
   "source": [
    "Thus features 3 and 4 (numcart, numview) are the top two most important features (according to the Random Forest model). We'll use these two features, along with interaction time (feature 1) for visualization of the data, but we'll fit classification models using all 6 of the features. Let's set up a convenient function for the visualization with the numcart and numview features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oQLECIbPKzD"
   },
   "outputs": [],
   "source": [
    "# This function visualizes the classification output on a scatter plot,\n",
    "# indicating the 4 types of outputs in a confusion matrix \n",
    "# (True Positive, True Negative, False Negative, False Positive)\n",
    "from mpl_toolkits import mplot3d\n",
    "def visualize_scatter_plot(X, y, yhat, title):\n",
    "    loc11 = np.where((y > 0) & (yhat > 0), 1, 0) #TP\n",
    "    loc00 = np.where((y == 0) & (yhat == 0), 1, 0) #TN\n",
    "    loc10 = np.where((y > 0) & (yhat == 0), 1, 0) #FN\n",
    "    loc01 = np.where((y == 0) & (yhat > 0), 1, 0) #FP\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    ax.scatter3D(\n",
    "        X[np.where(loc00 > 0), 3],\n",
    "        X[np.where(loc00 > 0), 4],\n",
    "        X[np.where(loc00 > 0), 1],\n",
    "        color='blue',\n",
    "        marker='o')\n",
    "    ax.scatter3D(\n",
    "        X[np.where(loc10 > 0), 3],\n",
    "        X[np.where(loc10 > 0), 4],\n",
    "        X[np.where(loc10 > 0), 1],\n",
    "        color='cyan',\n",
    "        marker='o')\n",
    "    ax.scatter3D(\n",
    "        X[np.where(loc11 > 0), 3],\n",
    "        X[np.where(loc11 > 0), 4],\n",
    "        X[np.where(loc11 > 0), 1],\n",
    "        color='orange',\n",
    "        marker='^')\n",
    "    ax.scatter3D(\n",
    "        X[np.where(loc01 > 0), 3],\n",
    "        X[np.where(loc01 > 0), 4],\n",
    "        X[np.where(loc01 > 0), 1],\n",
    "        color='green',\n",
    "        marker='^')\n",
    "    ax.set_xlabel('NumCart')\n",
    "    ax.set_ylabel('NumViews')\n",
    "    ax.set_zlabel('InteractionTime')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqG60zCOPKzF"
   },
   "source": [
    "## Task 4: Build Logistic Regression and SVM models\n",
    "\n",
    "We will be fitting both a Logistic Regression and SVM model to the reduced features and then looking at classification metrics such as Accuracy, Precision, Recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecEMOdxyPKzG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qTOqA8oz107"
   },
   "source": [
    "1. Instantiate, train, and predict with the Logistic Regression model.\n",
    "\n",
    "    Make sure to account for the imbalanced classes with with `class_weight` parameter.\n",
    "\n",
    "    Remember to use the ***train*** data for building the model and the ***test*** data when making and evaluating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh5SBHJrPKzI"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "lr =  LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(X_train_reduced_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test_reduced_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5iHJRIC0Nfl"
   },
   "source": [
    "2. Calculate these classification metrics for the Logistic Regression model:\n",
    "\n",
    "    * accuracy\n",
    "    * precision\n",
    "    * recall\n",
    "    * f1 score\n",
    "    * confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHshtF_IPKzP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_classification_metrics(y_test, y_pred): \n",
    "    metrics_dict = {} \n",
    "    metrics_dict[\"ACC\"] = accuracy(y_test, y_pred)\n",
    "    metrics_dict[\"PREC\"]  = precision(y_test, y_pred)\n",
    "    metrics_dict[\"REC\"]  = recall(y_test, y_pred)\n",
    "    metrics_dict[\"REC\"]  = recall(y_test, y_pred)\n",
    "    metrics_dict[\"F1_SCORE\"]  =  f1_score(y_test, y_pred)\n",
    "    metrics_dict[\"CONFUSION_MATRIX\"]  = confusion_matrix(y_test, y_pred)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_classification_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# [YOUR CODE HERE]\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[43mget_classification_metrics\u001b[49m(y_test, y_pred))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_classification_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# [YOUR CODE HERE]\n",
    "pprint.pprint(get_classification_metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the `visualize_scatter_plot()` function to visualize the performance of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "# visualize_scatter_plot(X_test_reduced_normalized, y_test, y_pred, \"Logistic Regression model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv858jlV1d__"
   },
   "source": [
    "4. Instantiate, train and predict with the SVM mode.\n",
    "\n",
    "    Again, remember to account fo the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1BXMFM2PKzR"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X_train_reduced_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4QY6-p21o1X"
   },
   "source": [
    "5. Calculate the classification metrics for the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eixQil93PKzU"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]\n",
    "pprint.pprint(get_classification_metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use the `visualize_scatter_plot()` function to visualize the performance of the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZoEc46tPKzZ"
   },
   "source": [
    "## Task 5: Fit a Non-Linear Classifier, the Gradient Boosted Tree.\n",
    "\n",
    "1. Instantiate, train, and predict with the Gradient Boosted Trees model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhRgpHtyGnzD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLalaVS0rHJ9"
   },
   "source": [
    "2. Evaluate the model by calculating the classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7fQY3dAGnzD"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualize the performance with the `visualize_scatter_plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VBEOQ9pPKzg"
   },
   "source": [
    "## Task 6: Analyze importace of data sample balancing using a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0qpuZHw5A5I"
   },
   "source": [
    "1. Instantiate, train, predict with, and evaluate an unbalanced random forest classifier. (Follow the same steps you did above with the other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQGEtfE1PKzg"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8r4-4vmtegj"
   },
   "source": [
    "2. Repeat the previous exercise, but this time, incorporate class-balanced penalty weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5HietxwPKzm"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM6F1YQ6ADgJ"
   },
   "source": [
    "3. Visualize a Decision Tree. Spend some time inspecting this visualization of the tree--what does each line in the boxes mean? Discuss this with your partners. What does the \"value=...\" line indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8zENaJ9PKz1"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "export_graphviz(rf_model_b.estimators_[0], max_depth=5, out_file='tree.dot', \n",
    "                feature_names = selected_columns[:-1],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "Image(filename = 'tree.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week2_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
