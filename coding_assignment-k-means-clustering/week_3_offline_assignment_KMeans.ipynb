{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKrVvdWlGAFK"
   },
   "source": [
    "# In Depth: k-Means Clustering\n",
    "\n",
    "## Adapted from a tutorial which itself is based on an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas. The content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/PDSH-cover-small.png?raw=1\">\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edY8xTosGAFL"
   },
   "source": [
    "In the previous few sections, we have explored one category of unsupervised machine learning models: dimensionality reduction.\n",
    "Here we will move on to another class of unsupervised machine learning models: clustering algorithms.\n",
    "Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
    "\n",
    "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as *k-means clustering*, which is implemented in ``sklearn.cluster.KMeans``.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEOxAmMDGAFL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnKvyj5yGAFO"
   },
   "source": [
    "## Part 1: Introducing k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnRjA0qaGAFO"
   },
   "source": [
    "The *k*-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
    "It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
    "\n",
    "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "- Each point is closer to its own cluster center than to other cluster centers.\n",
    "\n",
    "Those two assumptions are the basis of the *k*-means model.\n",
    "We will soon dive into exactly *how* the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the *k*-means result.\n",
    "\n",
    "1. Let's generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization.\n",
    "\n",
    "    Look at the plot to get an understanding of the data. How many clusters do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_0T8f2eGAFP"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDlkcet6GAFS"
   },
   "source": [
    "By eye, you can pick out the clusters. The *k*-means algorithm does this automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rSiFiRgk94w"
   },
   "source": [
    "2. Use *k*-means with *k* = 4 on the synthetic data generated above.\n",
    "\n",
    "    Follow these steps to build the k-means model and use it to make predictions:\n",
    "    \n",
    "    * Instantiate a `KMeans` object with 4 clusters.\n",
    "    * Train the model on the dataset.\n",
    "    * Make predictions on the same dataset.\n",
    "    \n",
    "    Note: you can use the `.fit()` and `.predict()` methods separately as we have with supervised learning methods, or call the `.fit_predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlJa7HSBGAFT"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = None  # the KMeans model\n",
    "y_kmeans = None  # predictions\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbcvDzA1GAFW"
   },
   "source": [
    "3. Visualize the results by plotting the data colored by these labels. We will also plot the cluster centers as determined by the *k*-means estimator.\n",
    "\n",
    "    Do the clusters align with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG4u3kaqGAFW"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8iQwGBjGAFY"
   },
   "source": [
    "The good news is that the *k*-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n",
    "But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly.\n",
    "Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to *k*-means involves an intuitive iterative approach known as *expectation–maximization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAyregRLGAFZ"
   },
   "source": [
    "## Part 2: k-Means Algorithm: Expectation–Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knb7fgMXGAFZ"
   },
   "source": [
    "Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science.\n",
    "*k*-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here.\n",
    "In short, the expectation–maximization approach here consists of the following procedure:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "   1. *E-Step*: assign points to the nearest cluster center\n",
    "   2. *M-Step*: set the cluster centers to the mean \n",
    "\n",
    "Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to.\n",
    "The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
    "\n",
    "The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.\n",
    "\n",
    "We can visualize the algorithm as shown in the following figure.\n",
    "For the particular initialization shown here, the clusters converge in just three iterations.\n",
    "For an interactive version of this figure, refer to the code in [the Appendix](06.00-Figure-Code.ipynb#Interactive-K-Means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAP6Jc-TGAFa"
   },
   "source": [
    "![(run code in Appendix to generate image)](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.11-expectation-maximization.png?raw=1)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Expectation-Maximization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2TppszIGAFa"
   },
   "source": [
    "1. The *k*-Means algorithm is simple enough that we can write it in a few lines of code. Complete your own basic implementation here.\n",
    "\n",
    "    Follow this algorithm:\n",
    "    \n",
    "    ```\n",
    "    1. Randomly choose cluster centers\n",
    "        - Choose n_clusters points from X to be the initial cluster centers.\n",
    "        - Hint: use state.permutation() to first select the row indices and then select those rows from X\n",
    "    2. Loop until cluster centers stop moving:\n",
    "        - Assign labels to each datapoint in X based on the closest center\n",
    "            Hint: Use the pairwise_distances_argmin() function\n",
    "        - Calculate each new center to be the mean of the points in the cluster\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsoBgvkNGAFb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    state = np.random.RandomState(rseed)\n",
    "    # [YOUR CODE HERE]\n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Look at this plot of the clusters with their assignments based on your implementation. Does it get the same assignments as with scikit-learn's implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_V9vsOuGAFe"
   },
   "source": [
    "## Part 3: Caveats of expectation–maximization\n",
    "\n",
    "There are a few issues to be aware of when using the expectation–maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDG89xUhGAFf"
   },
   "source": [
    "#### The globally optimal result may not be achieved.\n",
    "First, although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the *global* best solution.\n",
    "For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results.\n",
    "\n",
    "1. Run your `find_clusters()` function again, but this time give the random seed of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNAIH54rGAFf"
   },
   "outputs": [],
   "source": [
    "centers, labels = None # [YOUR CODE HERE]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj3AGfC4GAFi"
   },
   "source": [
    "Here the E–M approach has converged, but has not converged to a globally optimal configuration. For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the ``n_init`` parameter, which defaults to 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqWoPIIDGAFi"
   },
   "source": [
    "#### The number of clusters must be selected beforehand.\n",
    "Another common challenge with *k*-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.\n",
    "For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters.\n",
    "\n",
    "2. Run your `find_clusters()` function with 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRwWBH2AGAFj"
   },
   "outputs": [],
   "source": [
    "centers, labels = None # [YOUR CODE HERE]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvDDClgkGAFl"
   },
   "source": [
    "Whether the result is meaningful is a question that is difficult to answer definitively; one approach that is rather intuitive, but that we won't discuss further here, is called [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
    "\n",
    "Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models; see [In Depth: Gaussian Mixture Models](05.12-Gaussian-Mixtures.ipynb)) or which *can* choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the ``sklearn.cluster`` submodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1SRnYkAGAFl"
   },
   "source": [
    "#### k-means is limited to linear cluster boundaries\n",
    "The fundamental model assumptions of *k*-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "\n",
    "In particular, the boundaries between *k*-means clusters will always be linear, which means that it will fail for more complicated boundaries.\n",
    "\n",
    "3. Run your `find_clusters()` function on this new dataset with `n_clusters=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kO-07XoGAFm"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)\n",
    "centers, labels = None # [YOUR CODE HERE]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpMO0O1bGAFr"
   },
   "source": [
    "This situation is reminiscent of the discussion on Support Vector Machines, where we used a kernel transformation to project the data into a higher dimension where a linear separation is possible.\n",
    "We might imagine using the same trick to allow *k*-means to discover non-linear boundaries.\n",
    "\n",
    "One version of this kernelized *k*-means is implemented in Scikit-Learn within the `SpectralClustering` estimator.\n",
    "It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a *k*-means algorithm.\n",
    "\n",
    "4. Use `SpectralClustering` to find the clusters for the dataset. Give it the arguments: `n_clusters=2`, `affinity='nearest_neighbors'`, and `assign_labels='kmeans'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSTYZtVPGAFr"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "labels = # [YOUR CODE HERE]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpckNqWMGAFt"
   },
   "source": [
    "We see that with this kernel transform approach, the kernelized *k*-means is able to find the more complicated nonlinear boundaries between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfgyNURQGAFu"
   },
   "source": [
    "#### k-means can be slow for large numbers of samples\n",
    "Because each iteration of *k*-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows.\n",
    "You might wonder if this requirement to use all data at each iteration can be relaxed; for example, you might just use a subset of the data to update the cluster centers at each step.\n",
    "This is the idea behind batch-based *k*-means algorithms, one form of which is implemented in ``sklearn.cluster.MiniBatchKMeans``.\n",
    "The interface for this is the same as for standard ``KMeans``; we will see an example of its use as we continue our discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oGXsiduGAGE"
   },
   "source": [
    "## Part 4: Example: *k*-means for color compression\n",
    "\n",
    "One interesting application of clustering is in color compression within images.\n",
    "For example, imagine you have an image with millions of colors.\n",
    "In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors.\n",
    "\n",
    "For example, consider the image shown in the following figure, which is from the Scikit-Learn ``datasets`` module (for this to work, you'll have to have the ``pillow`` Python package installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e68CfCbAGAGE"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(china);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjZVqsxVGAGH"
   },
   "source": [
    "The image itself is stored in a three-dimensional array of size `(height, width, RGB)`, containing red/blue/green contributions as integers from 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcNlbJsdGAGH"
   },
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf0ZXSizGAGK"
   },
   "source": [
    "One way we can view this set of pixels is as a cloud of points in a three-dimensional color space.\n",
    "\n",
    "1. Reshape the data to `[n_samples]` (number of pixels) by `[n_features]` (RGB), and rescale the colors so that they lie between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFvOtUa3GAGK"
   },
   "outputs": [],
   "source": [
    "data = None # [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf2SzEUVGAGO"
   },
   "source": [
    "2. The following code visualizes these pixels in this color space, using a subset of 10,000 pixels for efficiency.\n",
    "\n",
    "    Look at the code and the resulting plot. What is it showing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7CuWtGGGAGO"
   },
   "outputs": [],
   "source": [
    "def plot_pixels(data, title, colors=None, N=10000):\n",
    "    if colors is None:\n",
    "        colors = data\n",
    "    state = np.random.RandomState(0)\n",
    "    i = state.permutation(data.shape[0])[:N]\n",
    "    colors = colors[i]\n",
    "    R, G, B = data[i].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    ax[0].scatter(R, G, color=colors, marker='.')\n",
    "    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    ax[1].scatter(R, B, color=colors, marker='.')\n",
    "    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "    fig.suptitle(title, size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0HPtXxqGAGQ"
   },
   "outputs": [],
   "source": [
    "plot_pixels(data, title='Input color space: 16 million possible colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67ZN5k0-GAGT"
   },
   "source": [
    "Now let's reduce these 16 million colors to just 16 colors, using a *k*-means clustering across the pixel space.\n",
    "Because we are dealing with a very large dataset, we will use the mini batch *k*-means, which operates on subsets of the data to compute the result much more quickly than the standard *k*-means algorithm.\n",
    "\n",
    "3. Use scikit-learn's implementation of mini batch *k*-means to find 16 clusters. The new colors will be the cluster centers of the predictions of the *k*-means algorithm.\n",
    "\n",
    "    Hint: Use the `.cluster_centers_` attribute to find the cluster centers from the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc5Mfe0SGAGT"
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')  # Fix NumPy issues.\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "new_colors = None\n",
    "# [YOUR CODE HERE]\n",
    "\n",
    "plot_pixels(data, colors=new_colors, title=\"Reduced color space: 16 colors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M0g7POtGAGV"
   },
   "source": [
    "The result is a re-coloring of the original pixels, where each pixel is assigned the color of its closest cluster center.\n",
    "\n",
    "Plotting these new colors in the image space rather than the pixel space shows us the original image with just the 16 colors.\n",
    "\n",
    "4. Run the code below and compare the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCWW3l6JGAGW"
   },
   "outputs": [],
   "source": [
    "china_recolored = new_colors.reshape(china.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "ax[0].imshow(china)\n",
    "ax[0].set_title('Original Image', size=16)\n",
    "ax[1].imshow(china_recolored)\n",
    "ax[1].set_title('16-color Image', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUK9_kUxGAGY"
   },
   "source": [
    "Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable.\n",
    "This image on the right achieves a compression factor of around 1 million!\n",
    "While this is an interesting application of *k*-means, there are certainly better way to compress information in images.\n",
    "But the example shows the power of thinking outside of the box with unsupervised methods like *k*-means."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "3.1_K_Means.ipynb",
   "provenance": [
    {
     "file_id": "1Frx0N2835l-yTUPhb9ilxtbuN5f_fH35",
     "timestamp": 1603919176235
    },
    {
     "file_id": "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb",
     "timestamp": 1603836324444
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
